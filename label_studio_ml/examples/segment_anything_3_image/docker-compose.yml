version: "3.8"

services:
  ml-backend:
    container_name: sam3-ml-backend
    image: humansignal/ml-backend:sam3-v0
    build:
      context: .
      args:
        TEST_ENV: ${TEST_ENV}

    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

    environment:
      # specify these parameters if you want to use basic auth for the model server
      - BASIC_AUTH_USER=
      - BASIC_AUTH_PASS=
      # set the log level for the model server
      - LOG_LEVEL=DEBUG
      # specify the number of workers and threads for the model server
      - WORKERS=1
      - THREADS=4
      # specify the model directory (likely you don't need to change this)
      - MODEL_DIR=/data/models
      # specify device
      - DEVICE=cuda
      # HuggingFace token for downloading SAM3 checkpoint (required on first run)
      - HF_TOKEN=
      # OR specify a local checkpoint path (mounted via volumes below)
      # - SAM3_CHECKPOINT=/app/checkpoints/sam3.pt
      - NVIDIA_VISIBLE_DEVICES=all

      # Specify the Label Studio URL and API key to access
      # uploaded, local storage and cloud storage files.
      # Do not use 'localhost' as it does not work within Docker containers.
      # Use prefix 'http://' or 'https://' for the URL always.
      # Determine the actual IP using 'ifconfig' (Linux/Mac) or 'ipconfig' (Windows).
      - LABEL_STUDIO_URL=
      - LABEL_STUDIO_API_KEY=
    ports:
      - "9090:9090"
    volumes:
      - "./data/server:/data"
      # Uncomment to use a pre-downloaded checkpoint:
      # - "/path/to/your/checkpoints:/app/checkpoints"
